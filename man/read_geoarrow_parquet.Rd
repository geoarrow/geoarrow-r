% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/read.R
\name{read_geoarrow_parquet}
\alias{read_geoarrow_parquet}
\alias{read_geoarrow_feather}
\alias{read_geoarrow_ipc_stream}
\alias{read_geoarrow_parquet_sf}
\alias{read_geoarrow_feather_sf}
\alias{geoarrow_collect}
\alias{geoarrow_collect.Table}
\alias{geoarrow_collect.RecordBatch}
\alias{geoarrow_collect.RecordBatchReader}
\alias{geoarrow_collect.Dataset}
\alias{geoarrow_collect.arrow_dplyr_query}
\title{Read geometry from Apache Parquet files}
\usage{
read_geoarrow_parquet(
  file,
  ...,
  as_data_frame = TRUE,
  handler = NULL,
  metadata = NULL
)

read_geoarrow_feather(
  file,
  ...,
  as_data_frame = TRUE,
  handler = NULL,
  metadata = NULL
)

read_geoarrow_ipc_stream(
  file,
  ...,
  as_data_frame = TRUE,
  handler = NULL,
  metadata = NULL
)

read_geoarrow_parquet_sf(file, ...)

read_geoarrow_feather_sf(file, ...)

geoarrow_collect(x, ..., handler = NULL, metadata = NULL)

\method{geoarrow_collect}{Table}(x, ..., handler = NULL, metadata = NULL)

\method{geoarrow_collect}{RecordBatch}(x, ..., handler = NULL, metadata = NULL)

\method{geoarrow_collect}{RecordBatchReader}(x, trans = identity, ..., handler = NULL, metadata = NULL)

\method{geoarrow_collect}{Dataset}(x, trans = identity, ..., handler = NULL, metadata = NULL)

\method{geoarrow_collect}{arrow_dplyr_query}(x, ..., handler = NULL, metadata = NULL)
}
\arguments{
\item{file}{A file or InputStream to read; passed to
\code{\link[arrow:read_parquet]{arrow::read_parquet()}}, \code{\link[arrow:read_feather]{arrow::read_feather()}}, or
\code{\link[arrow:read_ipc_stream]{arrow::read_ipc_stream()}}.}

\item{...}{
  Arguments passed on to \code{\link[arrow:write_parquet]{arrow::write_parquet}}
  \describe{
    \item{\code{sink}}{A string file path, URI, or \link[arrow]{OutputStream}, or path in a file
system (\code{SubTreeFileSystem})}
    \item{\code{chunk_size}}{how many rows of data to write to disk at once. This
directly corresponds to how many rows will be in each row group in parquet.
If \code{NULL}, a best guess will be made for optimal size (based on the number of
columns and number of rows), though if the data has fewer than 250 million
cells (rows x cols), then the total number of rows is used.}
    \item{\code{version}}{parquet version, "1.0" or "2.0". Default "1.0". Numeric values
are coerced to character.}
    \item{\code{compression}}{compression algorithm. Default "snappy". See details.}
    \item{\code{compression_level}}{compression level. Meaning depends on compression algorithm}
    \item{\code{use_dictionary}}{Specify if we should use dictionary encoding. Default \code{TRUE}}
    \item{\code{write_statistics}}{Specify if we should write statistics. Default \code{TRUE}}
    \item{\code{data_page_size}}{Set a target threshold for the approximate encoded
size of data pages within a column chunk (in bytes). Default 1 MiB.}
    \item{\code{use_deprecated_int96_timestamps}}{Write timestamps to INT96 Parquet format. Default \code{FALSE}.}
    \item{\code{coerce_timestamps}}{Cast timestamps a particular resolution. Can be
\code{NULL}, "ms" or "us". Default \code{NULL} (no casting)}
    \item{\code{allow_truncated_timestamps}}{Allow loss of data when coercing timestamps to a
particular resolution. E.g. if microsecond or nanosecond data is lost when coercing
to "ms", do not raise an exception}
    \item{\code{properties}}{A \code{ParquetWriterProperties} object, used instead of the options
enumerated in this function's signature. Providing \code{properties} as an argument
is deprecated; if you need to assemble \code{ParquetWriterProperties} outside
of \code{write_parquet()}, use \code{ParquetFileWriter} instead.}
    \item{\code{arrow_properties}}{A \code{ParquetArrowWriterProperties} object. Like
\code{properties}, this argument is deprecated.}
  }}

\item{as_data_frame}{Use \code{FALSE} to return an \link[arrow:Table]{arrow::Table}
instead of a data.frame.}

\item{handler}{A \link[wk:wk_handle]{wk handler} to use when \code{as.data.frame}
is TRUE for all geometry columns.}

\item{metadata}{Optional metadata to include to override metadata available
in the file.}

\item{x}{An object to collect into a data.frame, converting geometry
columns according to \code{handler}.}

\item{trans}{A function to be applied to each chunk after it has been
collected into a data frame.}
}
\value{
The result of \code{\link[arrow:read_parquet]{arrow::read_parquet()}}, with geometry
columns processed according to \code{handler}.
}
\description{
Read geometry from Apache Parquet files
}
