% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/write.R
\name{write_geoparquet}
\alias{write_geoparquet}
\alias{write_geoparquet_feather}
\alias{write_geoparquet_ipc_stream}
\alias{as_geoarrow_table}
\title{Write geometry as Apache Arrow files}
\usage{
write_geoparquet(handleable, ..., schema = NULL, strict = FALSE)

write_geoparquet_feather(handleable, ..., schema = NULL, strict = FALSE)

write_geoparquet_ipc_stream(handleable, ..., schema = NULL, strict = FALSE)

as_geoarrow_table(
  handleable,
  schema = NULL,
  strict = FALSE,
  null_point_as_empty = FALSE,
  geoparquet_metadata = FALSE
)
}
\arguments{
\item{handleable}{An object with a \code{\link[wk:wk_handle]{wk::wk_handle()}} method}

\item{...}{
  Arguments passed on to \code{\link[arrow:write_parquet]{arrow::write_parquet}}
  \describe{
    \item{\code{x}}{\code{data.frame}, \link[arrow]{RecordBatch}, or \link[arrow]{Table}}
    \item{\code{sink}}{A string file path, URI, or \link[arrow]{OutputStream}, or path in a file
system (\code{SubTreeFileSystem})}
    \item{\code{chunk_size}}{how many rows of data to write to disk at once. This
directly corresponds to how many rows will be in each row group in parquet.
If \code{NULL}, a best guess will be made for optimal size (based on the number of
columns and number of rows), though if the data has fewer than 250 million
cells (rows x cols), then the total number of rows is used.}
    \item{\code{version}}{parquet version, "1.0" or "2.0". Default "1.0". Numeric values
are coerced to character.}
    \item{\code{compression}}{compression algorithm. Default "snappy". See details.}
    \item{\code{compression_level}}{compression level. Meaning depends on compression algorithm}
    \item{\code{use_dictionary}}{Specify if we should use dictionary encoding. Default \code{TRUE}}
    \item{\code{write_statistics}}{Specify if we should write statistics. Default \code{TRUE}}
    \item{\code{data_page_size}}{Set a target threshold for the approximate encoded
size of data pages within a column chunk (in bytes). Default 1 MiB.}
    \item{\code{use_deprecated_int96_timestamps}}{Write timestamps to INT96 Parquet format. Default \code{FALSE}.}
    \item{\code{coerce_timestamps}}{Cast timestamps a particular resolution. Can be
\code{NULL}, "ms" or "us". Default \code{NULL} (no casting)}
    \item{\code{allow_truncated_timestamps}}{Allow loss of data when coercing timestamps to a
particular resolution. E.g. if microsecond or nanosecond data is lost when coercing
to "ms", do not raise an exception}
    \item{\code{properties}}{A \code{ParquetWriterProperties} object, used instead of the options
enumerated in this function's signature. Providing \code{properties} as an argument
is deprecated; if you need to assemble \code{ParquetWriterProperties} outside
of \code{write_parquet()}, use \code{ParquetFileWriter} instead.}
    \item{\code{arrow_properties}}{A \code{ParquetArrowWriterProperties} object. Like
\code{properties}, this argument is deprecated.}
  }}

\item{schema}{A \code{\link[narrow:narrow_schema]{narrow::narrow_schema()}} to use as a storage method.}

\item{strict}{Use \code{TRUE} to respect choices of storage type, dimensions,
and CRS provided by \code{schema}. The default, \code{FALSE}, updates these values
to match the data.}

\item{null_point_as_empty}{Use \code{TRUE} when creating point arrays for Parquet
to work around a bug whereby null items cannot be re-read by Arrow Parquet
readers.}

\item{geoparquet_metadata}{Use \code{TRUE} to add GeoParquet metadata to the
output schema metadata.}
}
\value{
The result of \code{\link[arrow:write_parquet]{arrow::write_parquet()}}, invisibly
}
\description{
Write geometry as Apache Arrow files
}
